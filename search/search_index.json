{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Amateur blog This is the first blog I write. I would like to organize my ideas and my research projects. Topics Signal Processing mainly focused on Audio (part of my PhD work) Machine learning, with special focus on Deep learning Programming: Python, tensorflow, keras, matlab, ... Earth Science","title":"Home"},{"location":"#amateur-blog","text":"This is the first blog I write. I would like to organize my ideas and my research projects.","title":"Amateur blog"},{"location":"#topics","text":"Signal Processing mainly focused on Audio (part of my PhD work) Machine learning, with special focus on Deep learning Programming: Python, tensorflow, keras, matlab, ... Earth Science","title":"Topics"},{"location":"about/","text":"My Background","title":"About"},{"location":"about/#my-background","text":"","title":"My Background"},{"location":"audio/info/","text":"Info Second page My info","title":"Audio Signal Processing"},{"location":"audio/info/#info","text":"Second page","title":"Info"},{"location":"audio/info/#my-info","text":"","title":"My info"},{"location":"code/info/","text":"CODE JAX Autograd and XLA toguether for high-performance machine learning research. With Autograd Automatically differentiate native Python and NumPy functions. grad = Supports reverse mode differentiation (backpropagation) With XLA Compile and run NumPy programs on GPUs and TPUs. Compilation under the hood by default. Tensorflow tf.GradientTape(): For automatic differentiation computing the gradient of a computation with respect to its input variables. \"Tape\" records all the operations executed. Import the necessary packages 1 2 3 4 5 import tensorflow as tf from tensorflow.keras.optimizers import Adam from tensorflow.keras.initializers import RandomNormal from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential Create a dense neural network with Spectral channels as input to infer LAI values. 1 2 3 X_train , X_val , y_train , y_val = train_test_split ( X_spec , y_lai , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) Create the model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Hyperparameters batch_size = 16 epochs = 150 optimizer = Adam ( lr = 0.01 ) weight_init = RandomNormal () modelLoss = tf . keras . losses . MeanSquaredError () metricLoss = tf . keras . metrics . MeanSquaredError () val_mse_loss = tf . keras . metrics . MeanSquaredError () def get_net ( weight_init ): net = Sequential () net . add ( Dense ( 16 , kernel_initializer = weight_init , activation = 'relu' , input_dim = 10 )) net . add ( Dense ( 16 , kernel_initializer = weight_init , activation = 'relu' )) #kernel_initializer=weight_init net . add ( Dense ( 8 , kernel_initializer = weight_init , activation = 'relu' )) net . add ( Dense ( 1 )) return net Start the training process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # Load the defined DNN model model = get_net ( weight_init ) # Initialize control training values train_loss_results = [] validation_loss_results = [] stop = False last_improvement = 0 last_improvement_total = 0 epoch = 0 best_loss = 100 # Calculate batches for training and validation bat_per_epoch = math . floor ( len ( X_train ) / batch_size ) bat_per_epoch_val = math . floor ( len ( X_val ) / batch_size ) # Training loop while epoch < epochs and stop == False : #for epoch in range(epochs): #print(f'Epoch: {epoch}/{epochs}') epoch_train_loss_avg = tf . keras . metrics . Mean () epoch_val_loss_avg = tf . keras . metrics . Mean () # Run train batch loop for i in range ( 1 , bat_per_epoch ): n = i * batch_size X_batch = X_train [ n : n + batch_size ] #X_train[ids_train[n:n+batch_size]] y_batch = y_train [ n : n + batch_size ] #y_train.take(ids_train[n:n+batch_size]) with tf . GradientTape () as tape : # Make prediction pred_y = model ( X_batch ) # Calculate loss train_loss = modelLoss ( tf . expand_dims ( y_batch . values , axis = 1 ), pred_y ) # Calculate gradients model_gradients = tape . gradient ( train_loss , model . trainable_variables ) # Update model optimizer . apply_gradients ( zip ( model_gradients , model . trainable_variables )) # Keep history of loss values epoch_train_loss_avg . update_state ( train_loss ) #End training batch train_loss_results . append ( epoch_train_loss_avg . result ()) # Run a validation loop at the end of each training epoch for i in range ( bat_per_epoch_val ): n = i * batch_size X_batch = X_val [ n : n + batch_size ] #X_val[ids_val[n:n+batch_size]] y_batch = y_val [ n : n + batch_size ] #y_val.take(ids_val[n:n+batch_size]) # Make prediction val_pred = model ( X_batch ) val_loss = val_mse_loss ( tf . expand_dims ( y_batch . values , axis = 1 ), val_pred ) # Keep history of validation loss values epoch_val_loss_avg . update_state ( val_loss ) # Read out training results #(f'Validation loss: {val_loss}') #End validation batch validation_loss_results . append ( epoch_val_loss_avg . result ()) print ( f 'Epoch: { epoch } / { epochs } Loss: { epoch_train_loss_avg . result () } , Val_loss: { epoch_val_loss_avg . result () } ' ) # Custom callback functions if epoch_val_loss_avg . result () < best_loss : best_loss = epoch_val_loss_avg . result () last_improvement = 0 else : last_improvement += 1 # Reduce learning rate if validation loss does not decrease for 10 epochs if last_improvement > 10 : lr = optimizer . lr . numpy () optimizer . lr . assign ( lr / 10 ) print ( f 'No improvement, reduce learning rate { lr / 10 } ' ) last_improvement_total = last_improvement last_improvement = 0 # Stop training if validation loss does not decrease for 20 epochs if last_improvement_total > 20 : print ( f 'No improvement after 20 epochs. Stop training' ) stop = True epoch += 1","title":"Code"},{"location":"code/info/#code","text":"","title":"CODE"},{"location":"code/info/#jax","text":"Autograd and XLA toguether for high-performance machine learning research. With Autograd Automatically differentiate native Python and NumPy functions. grad = Supports reverse mode differentiation (backpropagation) With XLA Compile and run NumPy programs on GPUs and TPUs. Compilation under the hood by default.","title":"JAX"},{"location":"code/info/#tensorflow","text":"tf.GradientTape(): For automatic differentiation computing the gradient of a computation with respect to its input variables. \"Tape\" records all the operations executed. Import the necessary packages 1 2 3 4 5 import tensorflow as tf from tensorflow.keras.optimizers import Adam from tensorflow.keras.initializers import RandomNormal from tensorflow.keras.layers import Dense from tensorflow.keras.models import Sequential Create a dense neural network with Spectral channels as input to infer LAI values. 1 2 3 X_train , X_val , y_train , y_val = train_test_split ( X_spec , y_lai , train_size = 0.8 , test_size = 0.2 , random_state = 0 ) Create the model 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # Hyperparameters batch_size = 16 epochs = 150 optimizer = Adam ( lr = 0.01 ) weight_init = RandomNormal () modelLoss = tf . keras . losses . MeanSquaredError () metricLoss = tf . keras . metrics . MeanSquaredError () val_mse_loss = tf . keras . metrics . MeanSquaredError () def get_net ( weight_init ): net = Sequential () net . add ( Dense ( 16 , kernel_initializer = weight_init , activation = 'relu' , input_dim = 10 )) net . add ( Dense ( 16 , kernel_initializer = weight_init , activation = 'relu' )) #kernel_initializer=weight_init net . add ( Dense ( 8 , kernel_initializer = weight_init , activation = 'relu' )) net . add ( Dense ( 1 )) return net Start the training process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 # Load the defined DNN model model = get_net ( weight_init ) # Initialize control training values train_loss_results = [] validation_loss_results = [] stop = False last_improvement = 0 last_improvement_total = 0 epoch = 0 best_loss = 100 # Calculate batches for training and validation bat_per_epoch = math . floor ( len ( X_train ) / batch_size ) bat_per_epoch_val = math . floor ( len ( X_val ) / batch_size ) # Training loop while epoch < epochs and stop == False : #for epoch in range(epochs): #print(f'Epoch: {epoch}/{epochs}') epoch_train_loss_avg = tf . keras . metrics . Mean () epoch_val_loss_avg = tf . keras . metrics . Mean () # Run train batch loop for i in range ( 1 , bat_per_epoch ): n = i * batch_size X_batch = X_train [ n : n + batch_size ] #X_train[ids_train[n:n+batch_size]] y_batch = y_train [ n : n + batch_size ] #y_train.take(ids_train[n:n+batch_size]) with tf . GradientTape () as tape : # Make prediction pred_y = model ( X_batch ) # Calculate loss train_loss = modelLoss ( tf . expand_dims ( y_batch . values , axis = 1 ), pred_y ) # Calculate gradients model_gradients = tape . gradient ( train_loss , model . trainable_variables ) # Update model optimizer . apply_gradients ( zip ( model_gradients , model . trainable_variables )) # Keep history of loss values epoch_train_loss_avg . update_state ( train_loss ) #End training batch train_loss_results . append ( epoch_train_loss_avg . result ()) # Run a validation loop at the end of each training epoch for i in range ( bat_per_epoch_val ): n = i * batch_size X_batch = X_val [ n : n + batch_size ] #X_val[ids_val[n:n+batch_size]] y_batch = y_val [ n : n + batch_size ] #y_val.take(ids_val[n:n+batch_size]) # Make prediction val_pred = model ( X_batch ) val_loss = val_mse_loss ( tf . expand_dims ( y_batch . values , axis = 1 ), val_pred ) # Keep history of validation loss values epoch_val_loss_avg . update_state ( val_loss ) # Read out training results #(f'Validation loss: {val_loss}') #End validation batch validation_loss_results . append ( epoch_val_loss_avg . result ()) print ( f 'Epoch: { epoch } / { epochs } Loss: { epoch_train_loss_avg . result () } , Val_loss: { epoch_val_loss_avg . result () } ' ) # Custom callback functions if epoch_val_loss_avg . result () < best_loss : best_loss = epoch_val_loss_avg . result () last_improvement = 0 else : last_improvement += 1 # Reduce learning rate if validation loss does not decrease for 10 epochs if last_improvement > 10 : lr = optimizer . lr . numpy () optimizer . lr . assign ( lr / 10 ) print ( f 'No improvement, reduce learning rate { lr / 10 } ' ) last_improvement_total = last_improvement last_improvement = 0 # Stop training if validation loss does not decrease for 20 epochs if last_improvement_total > 20 : print ( f 'No improvement after 20 epochs. Stop training' ) stop = True epoch += 1","title":"Tensorflow"},{"location":"earthscience/databases/","text":"7 FREE sites for downloading satellite images USGS Landviewer Copernicus - \"Sentinels Scientific Datahub\" - ESA Sentinel hub NASA earthData search - NASA EOSDIS Remote PIXEL INPE image catalog IN-SITU databases NEON (National Ecological Observatory Network) Data from 81 field sites across the united states (47 terrestial + 34 aquatic) Products USGS Every one to two days MODIS Aqua and Terra spacecraft views the entire surface of the Earth. In here there is a table with all the MODIS (Moderate Resolution Imaging Spectroradiometer) products available. For example we can filter LAI and FPAR products. The result MCD15A[3,2]H is a collection of combined MODIS, Terra MODIS or Aqua MODIS. It has a spatial resolution of 500m and a Multi-day temporal resolution. Copernicus global land service Product: Leaf Area Index (LAI) Sensor: PROVA-V Spatial information: 300m - 1km Temporal coverage: Jan 2014 - present, 1999 - present url: portal output files: GeoTIFF if the product is clipped, otherwise is NETCDF Example of plotting LAI values from a downloaded product. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from netCDF4 import Dataset import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap data = Dataset ( r 'C:\\IRENE\\IPL WORK\\copernicus data\\c_gls_LAI-RT0_202004200000_GLOBE_PROBAV_V2.0.1.nc' ) #get all the data lats = data . variables [ 'lat' ][:] lons = data . variables [ 'lon' ][:] times = data . variables [ 'time' ][:] # MAP coordinates for SPAIN using ESPG reference mp = Basemap ( llcrnrlat = 34 , urcrnrlat = 45 , llcrnrlon =- 10 , urcrnrlon = 4 , resolution = 'i' , epsg = 4326 ) lat_x = np . where ( lats > 34 ) lat_y = np . where ( lats < 45 ) lon_x = np . where ( lons >- 10 ) lon_y = np . where ( lons < 4 ) long , lat = np . meshgrid ( lons [ lon_x [ 0 ][ 0 ]: lon_y [ 0 ][ - 1 ]], lats [ lat_y [ 0 ][ 0 ]: lat_x [ 0 ][ - 1 ]]) x , y = mp ( long , lat ) cmap = plt . cm . Greens c_scheme = mp . pcolor ( x , y , np . squeeze ( lais [ 0 , lat_y [ 0 ][ 0 ]: lat_x [ 0 ][ - 1 ], lon_x [ 0 ][ 0 ]: lon_y [ 0 ][ - 1 ]]), cmap = cmap ) #, mp . drawcoastlines () mp . drawstates () mp . drawcountries () cbar = mp . colorbar ( c_scheme , location = 'right' , pad = '10%' ) plt . title ( 'LAI values' ) plt . show ()","title":"Databases"},{"location":"earthscience/databases/#7-free-sites-for-downloading-satellite-images","text":"USGS Landviewer Copernicus - \"Sentinels Scientific Datahub\" - ESA Sentinel hub NASA earthData search - NASA EOSDIS Remote PIXEL INPE image catalog","title":"7 FREE sites for downloading satellite images"},{"location":"earthscience/databases/#in-situ-databases","text":"NEON (National Ecological Observatory Network) Data from 81 field sites across the united states (47 terrestial + 34 aquatic)","title":"IN-SITU databases"},{"location":"earthscience/databases/#products","text":"","title":"Products"},{"location":"earthscience/databases/#usgs","text":"Every one to two days MODIS Aqua and Terra spacecraft views the entire surface of the Earth. In here there is a table with all the MODIS (Moderate Resolution Imaging Spectroradiometer) products available. For example we can filter LAI and FPAR products. The result MCD15A[3,2]H is a collection of combined MODIS, Terra MODIS or Aqua MODIS. It has a spatial resolution of 500m and a Multi-day temporal resolution.","title":"USGS"},{"location":"earthscience/databases/#copernicus-global-land-service","text":"Product: Leaf Area Index (LAI) Sensor: PROVA-V Spatial information: 300m - 1km Temporal coverage: Jan 2014 - present, 1999 - present url: portal output files: GeoTIFF if the product is clipped, otherwise is NETCDF Example of plotting LAI values from a downloaded product. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 from netCDF4 import Dataset import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.basemap import Basemap data = Dataset ( r 'C:\\IRENE\\IPL WORK\\copernicus data\\c_gls_LAI-RT0_202004200000_GLOBE_PROBAV_V2.0.1.nc' ) #get all the data lats = data . variables [ 'lat' ][:] lons = data . variables [ 'lon' ][:] times = data . variables [ 'time' ][:] # MAP coordinates for SPAIN using ESPG reference mp = Basemap ( llcrnrlat = 34 , urcrnrlat = 45 , llcrnrlon =- 10 , urcrnrlon = 4 , resolution = 'i' , epsg = 4326 ) lat_x = np . where ( lats > 34 ) lat_y = np . where ( lats < 45 ) lon_x = np . where ( lons >- 10 ) lon_y = np . where ( lons < 4 ) long , lat = np . meshgrid ( lons [ lon_x [ 0 ][ 0 ]: lon_y [ 0 ][ - 1 ]], lats [ lat_y [ 0 ][ 0 ]: lat_x [ 0 ][ - 1 ]]) x , y = mp ( long , lat ) cmap = plt . cm . Greens c_scheme = mp . pcolor ( x , y , np . squeeze ( lais [ 0 , lat_y [ 0 ][ 0 ]: lat_x [ 0 ][ - 1 ], lon_x [ 0 ][ 0 ]: lon_y [ 0 ][ - 1 ]]), cmap = cmap ) #, mp . drawcoastlines () mp . drawstates () mp . drawcountries () cbar = mp . colorbar ( c_scheme , location = 'right' , pad = '10%' ) plt . title ( 'LAI values' ) plt . show ()","title":"Copernicus global land service"},{"location":"earthscience/info/","text":"Earth Science Concepts related to Earth Science. NDVI: Normalized Difference Vegetation Index GPP: Gross Primary Productivity NPP: Net Primary Productivity APAR: Absorbed Photosynthetically Active Radiation FPAR: Fractional PAR FAPAR: Fraction of Absorbed PAR MODIS: Moderate Resolution Imaging Spectroradiometer LUE: Light Use Efficiency VPD: Vapor Preassure Deficit LE: Latent heat flux ET: EvapoTranspiration LAI: Leaf Area Index FOV: Field Of View TOA: Top Of Atmosphere ETM: Enhanced Thematic Mapper OLI: Operational Land Imager LaSRC: Landsat Surface Reflectance Code LEDAPS: Landsat Ecosystem Disturbance Adaptive Processing System","title":"Info"},{"location":"earthscience/info/#earth-science","text":"Concepts related to Earth Science. NDVI: Normalized Difference Vegetation Index GPP: Gross Primary Productivity NPP: Net Primary Productivity APAR: Absorbed Photosynthetically Active Radiation FPAR: Fractional PAR FAPAR: Fraction of Absorbed PAR MODIS: Moderate Resolution Imaging Spectroradiometer LUE: Light Use Efficiency VPD: Vapor Preassure Deficit LE: Latent heat flux ET: EvapoTranspiration LAI: Leaf Area Index FOV: Field Of View TOA: Top Of Atmosphere ETM: Enhanced Thematic Mapper OLI: Operational Land Imager LaSRC: Landsat Surface Reflectance Code LEDAPS: Landsat Ecosystem Disturbance Adaptive Processing System","title":"Earth Science"},{"location":"earthscience/multispectral/","text":"Multispectral Remote Sensing Data GIS Standard of encoding Geographical Information into a computer file. File formats Name properties extension netCDF (Network Common Data Form) File format with climate and Forecast (CF) metadata for earth science data. Binary storage in open format with optional compression. .nc GeoTIFF TIFF format which includes GIS compatible georeferencing systems. Byte data [0, 255] and colour scale. .tif / .tiff TIFF (Tagged Image File Format) Computer file format for storing raster graphics images. .tif / .tiff Shapefile Standard for representing geospatial vector data. Describes geometries as points, polylines or polygons. .shp Data structures Xarray Default package for handling spatial-temporal-variable datasets. In the format (latitude x longitude x time x variable) GeoPandas To store data from shapefiles. Raster Data stored as a grid of values which are rendered on a map as pixels. Each pixel value represents an area on the Earth's surface. Each cell \u2192 pixel \u2192 area on the ground Resolution \u2192 area that each pixel represents on the ground. Calculate NDVI from Landsat crop image Import the necessary packages 1 import rasterio Load data 1 2 3 4 imagePath = 'Landsat_collect/LC080340322016072301T1-SC20180214145802/crop/' imageFiles = glob . glob ( imagePath + '*_band*.tif' ) band4 = rasterio . open ( imageFiles [ 3 ]) #red band5 = rasterio . open ( imageFiles [ 4 ]) #nir Generate nir and red objects as arrays in float64 format 1 2 nir = band5 . read ( 1 ) . astype ( 'float64' ) red = band4 . read ( 1 ) . astype ( 'float64' ) NDVI calculation, empty cells or nodata cells are reported as 0 1 2 3 4 5 ndvi = np . where ( ( nir + red ) == 0. , 0 , ( nir - red ) / ( nir + red ) ) Export ndvi image 1 2 3 4 5 6 7 8 9 ndviImage = rasterio . open ( 'ndviImage.tiff' , 'w' , driver = 'Gtiff' , width = band4 . width , height = band4 . height , count = 1 , crs = band4 . crs , transform = band4 . transform , dtype = 'float64' ) ndviImage . write ( ndvi , 1 ) ndviImage . close () Projection EPSG code = standard to name projections using a numerical code Pan-sharpening Some characteristics of some of the most used sensors Sensor Spatial resolution Temporal resolution Spectral resolution Landsat TM 30 m 16 days 7 bands MODIS 250 - 100 m 1 day 36 bands Pansharpening is a process of merging high-resolution panchromatic and lower resolution multispectral imagery to create a single high-resolution color image. Radiance to reflectance Radiance is the variable directly measured by temote sensing instruments. It is how much light the instrument 'sees' from the object being observed. Depends on the illumination, the orientation and position of the target and the path of the light through the atmosphere. Reflectance is the ratio of the amount of light leaving a a target to the amount of light striking the target. It is a property of the material being observed. Spectral response functions (SRFs) Spectral response describes the sensitivity of the photosensor to optical radiation of different wavelengths. Tutorials Earth Lab: Multispectral remote sensing data in python Differential calculus: python colab","title":"Multispectral"},{"location":"earthscience/multispectral/#multispectral-remote-sensing-data","text":"","title":"Multispectral Remote Sensing Data"},{"location":"earthscience/multispectral/#gis","text":"Standard of encoding Geographical Information into a computer file.","title":"GIS"},{"location":"earthscience/multispectral/#file-formats","text":"Name properties extension netCDF (Network Common Data Form) File format with climate and Forecast (CF) metadata for earth science data. Binary storage in open format with optional compression. .nc GeoTIFF TIFF format which includes GIS compatible georeferencing systems. Byte data [0, 255] and colour scale. .tif / .tiff TIFF (Tagged Image File Format) Computer file format for storing raster graphics images. .tif / .tiff Shapefile Standard for representing geospatial vector data. Describes geometries as points, polylines or polygons. .shp","title":"File formats"},{"location":"earthscience/multispectral/#data-structures","text":"","title":"Data structures"},{"location":"earthscience/multispectral/#xarray","text":"Default package for handling spatial-temporal-variable datasets. In the format (latitude x longitude x time x variable)","title":"Xarray"},{"location":"earthscience/multispectral/#geopandas","text":"To store data from shapefiles.","title":"GeoPandas"},{"location":"earthscience/multispectral/#raster","text":"Data stored as a grid of values which are rendered on a map as pixels. Each pixel value represents an area on the Earth's surface. Each cell \u2192 pixel \u2192 area on the ground Resolution \u2192 area that each pixel represents on the ground. Calculate NDVI from Landsat crop image Import the necessary packages 1 import rasterio Load data 1 2 3 4 imagePath = 'Landsat_collect/LC080340322016072301T1-SC20180214145802/crop/' imageFiles = glob . glob ( imagePath + '*_band*.tif' ) band4 = rasterio . open ( imageFiles [ 3 ]) #red band5 = rasterio . open ( imageFiles [ 4 ]) #nir Generate nir and red objects as arrays in float64 format 1 2 nir = band5 . read ( 1 ) . astype ( 'float64' ) red = band4 . read ( 1 ) . astype ( 'float64' ) NDVI calculation, empty cells or nodata cells are reported as 0 1 2 3 4 5 ndvi = np . where ( ( nir + red ) == 0. , 0 , ( nir - red ) / ( nir + red ) ) Export ndvi image 1 2 3 4 5 6 7 8 9 ndviImage = rasterio . open ( 'ndviImage.tiff' , 'w' , driver = 'Gtiff' , width = band4 . width , height = band4 . height , count = 1 , crs = band4 . crs , transform = band4 . transform , dtype = 'float64' ) ndviImage . write ( ndvi , 1 ) ndviImage . close ()","title":"Raster"},{"location":"earthscience/multispectral/#projection","text":"EPSG code = standard to name projections using a numerical code","title":"Projection"},{"location":"earthscience/multispectral/#pan-sharpening","text":"Some characteristics of some of the most used sensors Sensor Spatial resolution Temporal resolution Spectral resolution Landsat TM 30 m 16 days 7 bands MODIS 250 - 100 m 1 day 36 bands Pansharpening is a process of merging high-resolution panchromatic and lower resolution multispectral imagery to create a single high-resolution color image.","title":"Pan-sharpening"},{"location":"earthscience/multispectral/#radiance-to-reflectance","text":"Radiance is the variable directly measured by temote sensing instruments. It is how much light the instrument 'sees' from the object being observed. Depends on the illumination, the orientation and position of the target and the path of the light through the atmosphere. Reflectance is the ratio of the amount of light leaving a a target to the amount of light striking the target. It is a property of the material being observed.","title":"Radiance to reflectance"},{"location":"earthscience/multispectral/#spectral-response-functions-srfs","text":"Spectral response describes the sensitivity of the photosensor to optical radiation of different wavelengths.","title":"Spectral response functions (SRFs)"},{"location":"earthscience/multispectral/#tutorials","text":"Earth Lab: Multispectral remote sensing data in python Differential calculus: python colab","title":"Tutorials"},{"location":"hybrid/intro/","text":"Hybrid modelling 1 2 3 !!! note \"Definition\" It combines the strengths of physical modelling (knowledge driven, physically interpretable extrapolation possible) and machine learning (data driven, less prior knowledge). Advantages: Adds prior knowledge Interpretable latent variables Objectives: Improving predictions Parametrization Forward solving partial differential equations Inverse modelling Discovering governing equations Methods: Physics-guide learning Physics-based loss: current approach Auxiliary task in multi-task learning Physics guide initialization (transfer learning) Physics-guide architecture Residual modelling Hybrid physics-ML models Multi-task learning Multi-task loss function which can learn to balance various regression and classification losses. Benefit: It can improve accuracy over separately trained models. Improves generalisation by sharing domain information between complimentary tasks. Examples: Fine-tuning: here, different learning tasks are used as a pre-training step. Learn unsupervised features from various data sources with an auto-encoder. L_{total} = \\sum_i w_i L_i L_{total} = \\sum_i w_i L_i","title":"Intro"},{"location":"hybrid/intro/#hybrid-modelling","text":"1 2 3 !!! note \"Definition\" It combines the strengths of physical modelling (knowledge driven, physically interpretable extrapolation possible) and machine learning (data driven, less prior knowledge). Advantages: Adds prior knowledge Interpretable latent variables Objectives: Improving predictions Parametrization Forward solving partial differential equations Inverse modelling Discovering governing equations Methods: Physics-guide learning Physics-based loss: current approach Auxiliary task in multi-task learning Physics guide initialization (transfer learning) Physics-guide architecture Residual modelling Hybrid physics-ML models","title":"Hybrid modelling"},{"location":"hybrid/intro/#multi-task-learning","text":"Multi-task loss function which can learn to balance various regression and classification losses. Benefit: It can improve accuracy over separately trained models. Improves generalisation by sharing domain information between complimentary tasks. Examples: Fine-tuning: here, different learning tasks are used as a pre-training step. Learn unsupervised features from various data sources with an auto-encoder. L_{total} = \\sum_i w_i L_i L_{total} = \\sum_i w_i L_i","title":"Multi-task learning"},{"location":"hybrid/theory/","text":"Theory Latent variables : variables that are not directly observed. They are inferred from other variables that are observed. Useful to reduce dimensionality of data. MMD (Maximum Mean Discrepancy): distance between feature means KLD (Kullback-Leibler Divergence): Scores how one distribution differs from another. RKHS (Reproducing Kernel Hilbert Spaces) KRR (Kernel Ridge Regression): Kernel method. Ridge regression: particular case of a quadratic regularized.","title":"Concepts"},{"location":"hybrid/theory/#theory","text":"Latent variables : variables that are not directly observed. They are inferred from other variables that are observed. Useful to reduce dimensionality of data. MMD (Maximum Mean Discrepancy): distance between feature means KLD (Kullback-Leibler Divergence): Scores how one distribution differs from another. RKHS (Reproducing Kernel Hilbert Spaces) KRR (Kernel Ridge Regression): Kernel method. Ridge regression: particular case of a quadratic regularized.","title":"Theory"}]}